{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Hotel Ratings on Tripadvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will practice web scraping. Let's get some basic information for each hotel in Boston."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each hotel page, scrape the Traverler ratings. **(10 pts)**\n",
    "\n",
    "![Information to be scraped](traveler_ratings.png)\n",
    "\n",
    "Save the data in \"traverler_ratings.csv\" in the following format:\n",
    "\n",
    "hotel_name, rating, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\y\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next url is /Hotels-g60745-oa30-Boston_Massachusetts-Hotels.html#ACCOM_OVERVIEW\n",
      "Next url is /Hotels-g60745-oa60-Boston_Massachusetts-Hotels.html#ACCOM_OVERVIEW\n",
      "We reached last page\n",
      "# of hotels: 82\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import csv\n",
    "import time \n",
    "import requests\n",
    "\n",
    "base_url = \"http://www.tripadvisor.com\"\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.76 Safari/537.36\"\n",
    "\n",
    "\"\"\" STEP 3 \"\"\"\n",
    "def get_hotellist_page(city_url, count):\n",
    "    \"\"\" Get the hotel list page given the url returned by\n",
    "        get_city_page(). Return the html after saving\n",
    "        it to the datadir \n",
    "    \"\"\"\n",
    "\n",
    "    url = base_url + city_url\n",
    "    # Sleep 0.5 sec before starting a new http request\n",
    "    time.sleep(0.5)\n",
    "    # Given the url, request the HTML page\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    # Save the file\n",
    "    with open('hotelist-' + str(count) + '.html', \"w\", encoding='utf-8') as h:\n",
    "         h.write(html)\n",
    "    return html\n",
    "    \n",
    "def get_page(page_url):\n",
    "    \n",
    "    ''' Get the hotel review page given the url provided by \n",
    "        listing title in hotellist_page. Return the html\n",
    "    '''\n",
    "    # Sleep 0.5 sec before starting a new http request\n",
    "    time.sleep(0.5)\n",
    "    url = base_url + page_url\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    return html\n",
    "\n",
    "\"\"\" STEP 4 \"\"\"\n",
    "def parse_hotellist_page(html):\n",
    "    \"\"\" Parse the html pages returned by get_hotellist_page().\n",
    "        Return the next url page to scrape (a city can have\n",
    "        more than one page of hotels) if there is, else exit\n",
    "        the script.\n",
    "    \"\"\"\n",
    "    global continues\n",
    "    global data\n",
    "    global hotel_count\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    # writing hotel name, rating, count to csv\n",
    "    with open('traverler_ratings.csv', 'a') as h:\n",
    "        headers = ['hotel_name', 'rating','count']\n",
    "        writer = csv.DictWriter(h, fieldnames=headers, lineterminator='\\n')\n",
    "    \n",
    "        # Extract hotel name, ratings and count for that rating\n",
    "        hotel_boxes = soup.select('div.listing.easyClear.p13n_imperfect')\n",
    "        #hotel_boxes = soup.find_all(\"div\", { \"class\" : \"listing_title\"})\n",
    "        hotel_count += len(hotel_boxes)\n",
    "        for hotel_box in hotel_boxes:\n",
    "            #hotel_name = hotel_box.find(text=True)\n",
    "            hotel_name = hotel_box.find('div', {'class' :'listing_title'}).find(text=True)\n",
    "            hotel_url = hotel_box.find('a').get('href')\n",
    "            data.append(hotel_url)\n",
    "            hotel_html = get_page(hotel_url)\n",
    "            soup2 = BeautifulSoup(hotel_html)\n",
    "            \n",
    "            # retrieving the travel ratings\n",
    "            cols_rating = soup2.find(id=\"ratingFilter\")\n",
    "            rows = cols_rating.find_all('li')\n",
    "            for r in rows:\n",
    "                rating = r.find(\"div\", {\"class\" : \"row_label\"}).get_text()\n",
    "\n",
    "                # find a list of all span elements\n",
    "                spans = r.find_all('span')\n",
    "                count = ''\n",
    "                for span in spans:\n",
    "                    # removing '\\n' from text                        \n",
    "                    stripped_txt = (span.text).rstrip()\n",
    "                    if (stripped_txt != ''):\n",
    "                        count = stripped_txt\n",
    "                        break\n",
    "\n",
    "                # writing to csv\n",
    "                writer.writerow({'hotel_name': hotel_name, 'rating': rating, 'count': count})\n",
    "\n",
    "    # Get next URL page if exists, else exit\n",
    "    div = soup.find(\"div\", {\"class\" : \"unified pagination standard_pagination\"})\n",
    "    # check if last page\n",
    "    if div.find('span', {'class' : 'nav next ui_button disabled'}):\n",
    "        print(\"We reached last page\")    \n",
    "        continues = False\n",
    "    else:\n",
    "        # If it is not last page there must be the Next URL\n",
    "        hrefs = div.findAll('a', href= True)\n",
    "        for href in hrefs:\n",
    "            if href.find(text = True) == 'Next':\n",
    "                print(\"Next url is %s\" % href['href'])\n",
    "                return href['href']\n",
    "\n",
    "# Get URL to obtain the list of hotels in a specific city\n",
    "city_url = '/Hotels-g60745-Boston_Massachusetts-Hotels.html'\n",
    "c=0\n",
    "\n",
    "# creating the csv file\n",
    "with open('traverler_ratings.csv', 'w') as h:\n",
    "        headers = ['hotel_name', 'rating','count']\n",
    "        writer = csv.DictWriter(h, fieldnames=headers, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "\n",
    "# global variable to keep track of whether continuing or not with the while loop\n",
    "continues = True\n",
    "# global variable list containing all hotel urls\n",
    "data = []\n",
    "hotel_count = 0\n",
    "\n",
    "while(continues):\n",
    "    c +=1\n",
    "    html = get_hotellist_page(city_url,c)\n",
    "    city_url = parse_hotellist_page(html)\n",
    "print('# of hotels: {}'.format(hotel_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, scrape all the reviews of each hotel for the star ratings of the following attributes: Value, Location, Sleep Quality, Rooms, Cleanliness, Service. Note that some reviews may not have attribute ratings and some may only have some of the attributes. **(25 pts)**\n",
    "\n",
    "![Information to be scraped](attribute_ratings.png)\n",
    "\n",
    "Save the data in \"attribute_ratings.csv\" in the following format:\n",
    "\n",
    "hotel_name, review_id, attribute, star_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\y\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next url is /Hotel_Review-g60745-d94344-Reviews-or10-Marriott_Vacation_Club_Pulse_at_Custom_House_Boston-Boston_Massachusetts.html#REVIEWS\n"
     ]
    }
   ],
   "source": [
    "def get_page_fast(page_url):\n",
    "    \n",
    "    ''' No delay in between http requests\n",
    "    '''\n",
    "    url = base_url + page_url\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    return html\n",
    "\n",
    "\"\"\" STEP 4 \"\"\"\n",
    "def parse_reviewlist_page(html):\n",
    "    \"\"\" Parse the review pages returned by get_page().\n",
    "        Return the next url page to scrape (a city can have\n",
    "        more than one page of hotels) if there is, else exit\n",
    "        the script.\n",
    "    \"\"\"\n",
    "    global continues\n",
    "    # for output testing\n",
    "    global review_count\n",
    "    \n",
    "    soup = BeautifulSoup(html)\n",
    "    #print(soup.prettify())\n",
    "    \n",
    "    # writing hotel name, rating, count to csv\n",
    "    with open('attribute_ratings.csv', 'a') as h:\n",
    "        headers = ['hotel_name', 'review_id','attribute', 'star_value']\n",
    "        writer = csv.DictWriter(h, fieldnames=headers, lineterminator='\\n')\n",
    "    \n",
    "        # Extract hotel name, review id, attributes and count for that attribute for all reviews of that hotel\n",
    "        hotel_name = soup.find(id=\"HEADING\").get_text()\n",
    "        review_boxes = soup.find_all(\"div\", { \"class\" : \"reviewSelector\"})\n",
    "        review_count += len(review_boxes)\n",
    "        for review_box in review_boxes:\n",
    "            \n",
    "            # retrieving attributes of div for review id\n",
    "            review_attrib = review_box.attrs\n",
    "            review_id = review_attrib['id']         \n",
    "\n",
    "            # retrieving review url for attributes pg \n",
    "            quote = review_box.find('div', {'class': 'quote'})\n",
    "            if (quote != None):\n",
    "                quote_url = quote.find('a').get('href')\n",
    "            \n",
    "                # getting review page of review and extracting attributes for featured review\n",
    "                quote_html = get_page_fast(quote_url)\n",
    "                soup2 = BeautifulSoup(quote_html)\n",
    "                feat_review = soup2.find(id=review_id)\n",
    "                \n",
    "                # extracting rating-list section\n",
    "                rating_list = feat_review.find('div', {'class': 'rating-list'})\n",
    "                \n",
    "                if (rating_list != None):\n",
    "                    # check if review has ratings\n",
    "                    if not (rating_list.find('span', {'class': 'noRatings'})):\n",
    "                        # get all the containers for review ratings\n",
    "                        recommend_ans = rating_list.find_all('li', {'class': 'recommend-answer'})\n",
    "                        for ans in recommend_ans:\n",
    "                            sprite_rating = ans.find('span', {'class': 'rate'}) \n",
    "                            # retrieving attribute text\n",
    "                            attribute = ans.find('div').get_text()\n",
    "                            # retrieving star value from img alt data\n",
    "                            temp = sprite_rating.find('img')\n",
    "                            star_val = temp['alt']\n",
    "\n",
    "                            # writing to csv\n",
    "                            writer.writerow({'hotel_name': hotel_name[2:-1], 'review_id': review_id[7:], 'attribute': attribute, 'star_value': star_val[0]})            \n",
    "\n",
    "    # Get next URL page if exists, else exit\n",
    "    div = soup.find(\"div\", {\"class\" : \"unified pagination \"})\n",
    "    # check if last page\n",
    "    if div.find('span', {'class' : 'nav next disabled'}):\n",
    "        print(\"We reached last page\")    \n",
    "        continues = False\n",
    "    else:\n",
    "        # If it is not last page there must be the Next URL\n",
    "        hrefs = div.findAll('a', href= True)\n",
    "        for href in hrefs:\n",
    "            if href.find(text = True) == 'Next':\n",
    "                print(\"Next url is %s\" % href['href'])\n",
    "                return href['href']\n",
    "\n",
    "\n",
    "# creating the csv file\n",
    "with open('attribute_ratings.csv', 'w') as h:\n",
    "    headers = ['hotel_name', 'review_id','attribute', 'star_value']\n",
    "    writer = csv.DictWriter(h, fieldnames=headers, lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "\n",
    "# Get hotel page of urls in data\n",
    "for hotel_url in data:\n",
    "    # keeps track of total english reviews for a hotel\n",
    "    review_count = 0\n",
    "    # global variable to keep track of whether continuing or not with the while loop\n",
    "    continues = True\n",
    "    while(continues):\n",
    "        hotel_html = get_page(hotel_url)\n",
    "        hotel_url = parse_reviewlist_page(hotel_html)\n",
    "    print('# of english reviews: {}'.format(review_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
